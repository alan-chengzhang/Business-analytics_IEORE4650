{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 4 (IEOR E4650 Fall 2019).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s5izpSul0F9",
        "colab_type": "text"
      },
      "source": [
        "# **IEOR E4650  Business Analytics (Fall 2019)**\n",
        "\n",
        "##**Lecture 4: Building a linear regression model**\n",
        "\n",
        "In this lecture, we discuss different ways of variable transformation to help improve the prediction power of our model.\n",
        "\n",
        "Learning objective:\n",
        "\n",
        "* Understand different ways to transform the variables.\n",
        "* Understand how to apply different techniques in the real-world settings.  \n",
        "* Understand how to use Python to estimate a model that includes transformed variables. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66lEaM8Oc2Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "link=\"https://drive.google.com/open?id=17Sa-DuRFCWfPzCW6uRbPwxAyo1mQARUn\"\n",
        "_,id=link.split(\"=\")\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('myfile.csv')  \n",
        "import pandas as pd\n",
        "Sales = pd.read_csv('myfile.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-UNr3Jsc_4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.formula.api import ols\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "Sales= shuffle(Sales)\n",
        "Sales.replace(\"-\",np.nan,inplace=True)\n",
        "Sales=Sales.astype({\"GROSS_SQUARE_FEET\":\"float64\",\"YEAR_BUILT\":\"float64\",\"LAND_SQUARE_FEET\":\"float64\"}) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ykS0BmPbZ_u",
        "colab_type": "text"
      },
      "source": [
        "##  Linear regression model for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-w27qlAnWIc",
        "colab_type": "text"
      },
      "source": [
        "Besides a prediction model like the following:\n",
        "\n",
        "$y=\\beta_0+\\beta_1 x_1+ \\beta_2 x_2 +\\beta_3 x_3 +\\epsilon$\n",
        "\n",
        "Are there different ways to build a richer model?\n",
        "\n",
        "Yes, here are 4 most popular methods:\n",
        "\n",
        "* polynomial \n",
        "* log transformation\n",
        "* interaction term\n",
        "* converting a categorical variable into dummy variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOY7FHuZNa9z",
        "colab_type": "text"
      },
      "source": [
        "###Including Polynomial Terms\n",
        "\n",
        "If we suspect that the impact of a variable on the dependent variable is not linear, we can add polynomial terms.\n",
        "\n",
        "For example, we might want to use the amount of time a sales agent spent talking to clients to predict the performance. We might not expect the effect of time on the sales to be linear, but expect the marginal return of the time on the sales to be deminishing. In this case, we can construct a model as follows:\n",
        "\n",
        "$Sales=\\beta_0+\\beta_1*time+\\beta_2*time^2+\\epsilon$\n",
        "\n",
        "\n",
        "`ols(formula=\"y~x1+I(x1**2)+I(x1**3)\",data=S1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIKCo92SNarM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_0f-eIiWThC",
        "colab_type": "text"
      },
      "source": [
        "###Including Logarithm Terms\n",
        "\n",
        "Usually, if the distribution of a variable exhibits a right tail, it can be useful to take the log transformation. Otherwise, the extreme values (which we call outlier) might influece our prediction too much. \n",
        "\n",
        "For example, if we want to perform log-transformation on $x_2$, we can use\n",
        "$$y=\\beta_0+\\beta_1 x_1+\\beta_2 log(x_2)+\\epsilon$$\n",
        "\n",
        "\n",
        "In order to take log-transformation, we will need the range of the variable to be positive. If the range is non-negative, we can add a small value to that variable as a remedy, for example using $log(x+1)$.\n",
        "\n",
        "To perform log-transformation in Python, we can simply use\n",
        "\n",
        "`ols(formula=\"y~x1+np.log(x2)\",data=S1)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENLGMeLKR01P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP0fmnzCjuQ6",
        "colab_type": "text"
      },
      "source": [
        "One thing to notice is that if we performed log-transformation on the dependent variable. Once we finished our prediction, we can then exponentiate the predicted value. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRI4w8GLNHLv",
        "colab_type": "text"
      },
      "source": [
        "###Including  Interaction Terms\n",
        "\n",
        "When we expect that the impact of one variable on the depedent variable might be affected by the value of another variable, we can use the interaction term. For example, if we expect the influence of $x_1$ on $y$ varies based on the value of $x_2$, we might want to use \n",
        "\n",
        "$$y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon$$\n",
        "\n",
        "\n",
        "For example, we might want the following model to customer satisfaction level of our hotdog. We might use the number of condiments we provide and the number of sausage options we provide as the predictors. If you expect that consumers will care less about (or more) about the number of condiments we provide if we give them more sausage options, then we have a good reason to include the interaction term. \n",
        "\n",
        "\n",
        "To include an interaction term in Python, we can use x1:x2.\n",
        "\n",
        "`ols(formula=\"y~x1+x2+x1:x2\",data=S1)`\n",
        "\n",
        "when we have `x1`, `x2` and `x1:x2` are all in the model, we can equivalently use x1*x2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuYP5HBKNGuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgWYsL9NeZo",
        "colab_type": "text"
      },
      "source": [
        "###Converting Categorical Terms into Dummy Variables\n",
        "\n",
        "A variable might be categorical, which mean the value of that variable indicates which category an observation belongs to. \n",
        "\n",
        "* If a categorical variable can only take 0/1, indicating whether an observation belongs to the category or not, we also call it dummy variable.\n",
        "\n",
        "* If a categorical variable can take multiple values (say N categories), we can then need to tranform this variable into N-1 dummy variables and include them in the regression model. \n",
        "\n",
        "Suppose we have a categorical variable that can take three values, then we can run the following regression:\n",
        "\n",
        "$$y=\\beta_0+\\beta_1 category_2 +\\beta_2 category_3 +\\epsilon$$\n",
        "\n",
        "Notice that you can only include 2 (more generally N-1) categories, since the whether an observation belongs to the first category can be perfectly inferred from whether this observation belongs to the second and the third category or not.\n",
        "\n",
        "\n",
        "\n",
        "In our example, \"Borough\" is can be considered a categorical variable. In this case, we can convert a categorical variable into several dummy variables. Each dummy variable indicates whether an observation belongs to a specific category.\n",
        "\n",
        "Suppose x2 is a categorical variable, we can use `C(x2)` to convert this variable to a dummy variable.\n",
        "\n",
        "`ols(formula=\"y~x1+C(x2)\",data=S1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNCt8qP1UwyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saBsRglrXC93",
        "colab_type": "text"
      },
      "source": [
        "#Activity\n",
        "\n",
        "Based on the data we have (Feel free to use all 3000 observations or discard some), construct a model that gives the highest RMSE. "
      ]
    }
  ]
}