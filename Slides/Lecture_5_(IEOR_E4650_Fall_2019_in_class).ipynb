{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 5 (IEOR E4650 Fall 2019 in class).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s5izpSul0F9",
        "colab_type": "text"
      },
      "source": [
        "# **IEOR E4650  Business Analytics (Fall 2019)**\n",
        "\n",
        "##**Lecture 5: Building a Good Model**\n",
        "\n",
        "Linear regression is a commonly used technique when analyzing data. In this lecture, we discuss how to build a good model to address the questions. \n",
        "\n",
        "Learning objective:\n",
        "\n",
        "(1) Understand the modeling focus for correlation and prediction\n",
        "\n",
        "(2) Understand to importance of cross validation\n",
        "\n",
        "(3) Able to perform cross validation\n",
        "\n",
        "(4) Able to work on model selection algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLirS-Kb2yqD",
        "colab_type": "text"
      },
      "source": [
        "## Modeling focus \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5ojPPMdBmK",
        "colab_type": "text"
      },
      "source": [
        "Again, let's return to two focuses of modeling:\n",
        "\n",
        "$$y=\\beta_0+\\beta_1 x_1 + \\epsilon $$\n",
        "\n",
        "* Correlation/causation\n",
        "\n",
        "We would like to have a model that allows us to do a good inference on $\\beta_1 $.\n",
        "\n",
        "* Prediction\n",
        "\n",
        "We would like to have a model that gives us good prediction \n",
        "\n",
        "\n",
        "Both correlation and prediction require us to \"correctly\" specify the model. \n",
        "> Is the data clean?\n",
        "\n",
        "> Is the model correctly specified? \n",
        "\n",
        ">>Do we need to take variable transformations? \n",
        "\n",
        ">>Is linear model a good choice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZvWelqVpAI",
        "colab_type": "text"
      },
      "source": [
        "### Special Focus for Causal inference. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tqb2K-bdGwi",
        "colab_type": "text"
      },
      "source": [
        " (1) Include factors that impact both $x_1$ and $y$, otherwise, $\\beta_1$ will be biased.  \n",
        "\n",
        "  >>For example, if we want to run the following model\n",
        "  \n",
        "  $$\\text{Spending_on_clothing}=\\beta_0+\\beta_1 \\text{Spending_on_grocery}+ \\epsilon$$ \n",
        "\n",
        "  >>We might find $\\beta_1$ positive and significant. However, does this mean spending more on grocery lead to more spending on clothing? Well, we are at least over-estimating the influence, because we are missing an impact factor, which is income. Maybe it is also a good idea to add the number of family members in the model. A better model to use is \n",
        "\n",
        "  $$\\text{Spending_on_clothing}=\\beta_0+\\beta_1 \\text{Spending_on_grocery}+  \\beta_2 \\text{Income} + \\beta_3 \\text{Family_members}  + \\epsilon$$ \n",
        "\n",
        ">> Here, \"Income\" and \"Family_members\" are called control variables.\n",
        "\n",
        "  >>Of course,we also want to check if variable transformations are needed. For example, it is very likely to see a right-skewed distribution for `Spending_on_clothing`, `Spending_on_grocery`, and `Income`. Check the histogram and make a decision. \n",
        "\n",
        "  >(2) If we have any factors that influence only the dependent variable, but not our independent variable, you should also include it in your model. It will help with the p-value. \n",
        "\n",
        "We will revisit the causal inference in the later lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uwotjCZc1o5",
        "colab_type": "text"
      },
      "source": [
        "### Special Focus of Prediction \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vej0041xdNoJ",
        "colab_type": "text"
      },
      "source": [
        "A good model is the one that has good prediction power on a set of samples that we never touched on before.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gWITUfrNFZ",
        "colab_type": "text"
      },
      "source": [
        "####Bias-Variance Trade-off\n",
        "\n",
        "As model goes from simple to complicated, we risk the danger of moving from underfitting to overfitting. \n",
        "* Underfitting: miss the relevant relations between features and target outputs. \n",
        "\t-> Too much “bias”\n",
        "\n",
        "\"bias\" means our prediction is not close enough to the actual value. \n",
        "\n",
        "* Overfitting: starts to model the random noise in the training data\n",
        "\t-> Too much “variance”\n",
        "\n",
        "\"variance\" means our prediction has lots of variation.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://i.stack.imgur.com/8RlJk.png\n",
        "\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "[Source: stackexchange.]\n",
        "\n",
        "Thus, our task is to find a model that balances variance and bias.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://www.researchgate.net/profile/Peter_Thesling2/publication/281821310/figure/fig2/AS:391521762332674@1470357470701/The-Bias-Variance-Tradeoff.png\n",
        "\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "[Source: ResearchGate]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gebjoNYe3nJ8",
        "colab_type": "text"
      },
      "source": [
        "#### Solution: Cross-Validation\n",
        "\n",
        "General Idea:  Using the same data for both model training and model evaluating can lead to over-fitting. Thus, we will have separate dataset for model training and the evaluation of the model accuracy.\n",
        "\n",
        "\n",
        "Split our data into three parts:\n",
        "\n",
        "* Training\n",
        "* Validation\n",
        "* Test\n",
        "\n",
        "Go through the following model training process\n",
        "\n",
        "* Step 1: Loop for each candidate model:\n",
        "\n",
        "> Use training set to train the model\n",
        "\n",
        "> Calculate the accuracy on validation set\n",
        "\n",
        "* Step 2: Choose the model with the highest accuracy on the validation set\n",
        "\n",
        "* Step 3: Report the performance of the final model using test set\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/1484/1*OJVhBtg5YgeW7rKXoxKQxg.png\n",
        "\" width=\"350\"/>\n",
        "</div>\n",
        "\n",
        "[Source: Medium]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQTQThuVcZfa",
        "colab_type": "text"
      },
      "source": [
        "#### Example of model selection:\n",
        "\n",
        "Suppose we would like to select a good model for predicting the sales.\n",
        "\n",
        "Which model is a good model?\n",
        "\n",
        " $$SalePrice=\\beta_0+\\beta_1 GrossSquareFeet + \\epsilon $$\n",
        "\n",
        " $$SalePrice=\\beta_0+\\beta_1 GrossSquareFeet+\\beta_2 GrossSquareFeet^2 + \\epsilon $$\n",
        "\n",
        "  $$log(SalePrice)=\\beta_0+\\beta_1 log(GrossSquareFeet) + \\epsilon $$\n",
        "\n",
        "Let's do the following three steps:\n",
        "\n",
        "1. Split the data into Part 1 (Training), Part 2 (Validation), and Part 3 (Testing)\n",
        "2. Run three models on Part 1 (Training)\n",
        "3. Test the model performances on Part 2 (Validation)\n",
        "4. Select the best model \n",
        "5. Report the model performance on Part 3 (Testing)\n",
        "\n",
        "**In addition, you need to have a standard on what is considered a good model.**\n",
        "\n",
        "Root Mean Square Error (RMSE):\n",
        "\n",
        "$RMSE=\\sqrt{\\frac{\\sum(y_i-\\widehat{y_i})^2}{N}}$ ---> This one weights error for each point equally\n",
        "\n",
        "Root Mean Square Percentage Error (RMSPE) [An example can be seen [here](https://www.kaggle.com/c/rossmann-store-sales/overview/evaluation)]:\n",
        "\n",
        "$RMSPE=\\sqrt{\\frac{\\sum(\\frac{\\widehat{y_i}-y_i}{y_i})^2}{N}}$ ---> This one gives a lower weights on the errors for observations with larger actual values \n",
        "\n",
        "RMSPE is less sensitive to outliers in the testing/training data. However, is not exactly aligning with the OLS objective function, which essentially is minimizing RMSE on the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kzeFPnhcY-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "link=\"https://drive.google.com/open?id=17Sa-DuRFCWfPzCW6uRbPwxAyo1mQARUn\"\n",
        "_,id=link.split(\"=\")\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('myfile.csv')  \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "Sales = pd.read_csv('myfile.csv')\n",
        "from statsmodels.formula.api import ols\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i45ziY87fiO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Sales=Sales.replace(\"-\",np.nan)\n",
        "Sales=Sales.dropna()\n",
        "Sales.shape\n",
        "Sales.rename(columns={})\n",
        "Sales=Sales.astype({\"GROSS_SQUARE_FEET\":\"float64\",\"YEAR_BUILT\":\"float64\",\"LAND_SQUARE_FEET\":\"float64\"}) \n",
        "\n",
        "Sales=Sales.rename(columns={\"GROSS_SQUARE_FEET\":\"GSF\",\"LAND_SQUARE_FEET\":\"LSF\",\"TOTAL_UNITS\":\"TU\",\"COMMERCIAL_UNITS\":\"CU\",\"RESIDENTIAL_UNITS\":\"RU\",\"SALE_PRICE\":\"SP\",\"YEAR_BUILT\":\"YB\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlplwb7Lgvk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "Sales= shuffle(Sales)\n",
        "Sales_training= \n",
        "Sales_validation= \n",
        "Sales_testing= \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMS4i7kddu-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model training on the training set\n",
        "model1=\n",
        "model2=\n",
        "model3=\n",
        "\n",
        "#Selecting the best model based the validation set\n",
        "\n",
        "##First, get the predicted value \n",
        "Prediction1=model1.predict(Sales_validation)\n",
        "Prediction2=\n",
        "###Model 3 took used log transformation on the dependent variable. \n",
        "###Need to exponentiate the predicted value \n",
        "Prediction3=\n",
        "\n",
        "##Second, get the RMSPE and RMSE\n",
        "Actual=Sales_validation[\"SP\"]\n",
        "RMSPE1= \n",
        "RMSPE2= \n",
        "RMSPE3= \n",
        "RMSE1= \n",
        "RMSE2= \n",
        "RMSE3= \n",
        "\n",
        "print(RMSPE1,RMSPE2,RMSPE3)\n",
        "print(RMSE1,RMSE2,RMSE3)\n",
        "\n",
        "#Report the final performance of the chosen the model on the test set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoO7z6uX87ah",
        "colab_type": "text"
      },
      "source": [
        "## Model Selection Algorithm\n",
        "\n",
        "Besides relying on the expertise. We can also use some model selection algorithms to helps us choose a better model.\n",
        "\n",
        "Two commonly used model selection algorihm are:\n",
        "\n",
        "**(1) Step-wise model section**\n",
        "  This model selection method works on add or remove variables one at a time to until the accuracy on the hold-out samples starts drop. \n",
        "\n",
        "  For example, forward model section as follow: \n",
        "\n",
        "(1) Select a set of variables \n",
        "\n",
        "(2) Loop:\n",
        "  >Try each variable still left in the set\n",
        "\n",
        "  >Select the variable that leads to the highest accuracy on the hold-out samples\n",
        "\n",
        "(3) Break the loop if adding one additional variable leads to decrease in accuracy on the hold-out sample\n",
        "\n",
        "\n",
        "**(2) Regularization-based model section**\n",
        "\n",
        "Ridge regression, Lasso regression,and Elastic-net are examples of commonly used algorithms. \n",
        "\n",
        "The idea is to add a penalty for complicated models.\n",
        "\n",
        "**Optimization without penalty:**\n",
        "$$\\sum(y_i-\\widehat{y_i})^2$$\n",
        "\n",
        "**Ridge (L2 regularization):**\n",
        "\n",
        "$$\\sum(y_i-\\widehat{y_i})^2+\\alpha \\sum(\\widehat{\\beta_k})^2$$\n",
        "\n",
        "**Lasso (L1 regularization ):**\n",
        "\n",
        "$$\\sum(y_i-\\widehat{y_i})^2+\\alpha \\sum|\\widehat{\\beta_k}|$$\n",
        "\n",
        "Both ridge and lasso can help reduce overfit. In addition, Lasso regression can drive certain $\\widehat{\\beta}$s to 0, which works as a variable selection tool.\n",
        "\n",
        "**Elastic-Net**\n",
        "Elastic-net is a weighted combination of Ridge and Lasso:\n",
        "\n",
        "$$\\sum(y_i-\\widehat{y_i})^2+\\alpha( L \\sum|\\widehat{\\beta_k}|+(1-L) \\sum(\\widehat{\\beta_k})^2)$$\n",
        "\n",
        "Especially, Elastic-Net reduces to Lasso when L=1, while reduces to Ridge when L=0.\n",
        "\n",
        "To implement Elastic-net using statsmodel package,use  [fit_regularized()](http://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html) instead of fit(). Especially, fit_regularized() has two important arguments. \n",
        "* alpha: this corresponds to the $\\alpha$ in the formula\n",
        "* L1_wt: this corresponds to $L$ in the formula \n",
        "\n",
        "Here, $\\alpha$ and $L$ values can be tuned using cross-validation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBiL336t9Cdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}